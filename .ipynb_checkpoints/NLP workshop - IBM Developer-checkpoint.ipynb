{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A hands-on introduction to Natural Language Processing\n",
    "\n",
    "## Pre-requisites\n",
    "Please have the pre-requisites ready on your machine. \n",
    "\n",
    "![Typing cat gif](https://media.giphy.com/media/o0vwzuFwCGAFO/giphy.gif)\n",
    "\n",
    "```jupyter notebook``` opens up a Jupyter notebook in your browser at default port 8888."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "Natural Language Toolkit is the most popular collection of libraries and programs to do NLP. You can find more about it here: http://www.nltk.org/book/ch00.html. We will mainly be using NLTK to perform different tasks, along with a few other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignoring warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK comes pre-loaded with texts from the Project Gutenberg archive that you can use. It also has a collection of informal text from discussion forums, conversations, chat sessions, movie scripts, etc. NLTK has corpora in other languages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Printing the first 100 characters of each of the files\n",
    "for fileid in gutenberg.fileids():\n",
    "    print(fileid, gutenberg.raw(fileid)[:100], '...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "![Fetch data](https://media.giphy.com/media/4FQMuOKR6zQRO/giphy.gif)\n",
    "\n",
    "Data can come from a variety of sources in different formats. Natural language can be in the form of text or speech. For the purpose of this tutorial, we will be focusing on text-based processing as opposed to speech recognition and synthesis. Textual data can be stored in databases, dataframes, text files, webpages, etc. A list of text datasets can be found here: https://github.com/niderhoff/nlp-datasets. <br>\n",
    "\n",
    "Let's create a list with a few sentences that will serve as the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\"Today is 29th April. I am in San Francisco. Currently I am \" \\\n",
    "                \"attending a Natural Language Processing tutorial.\"]\n",
    "print('\\nSample data', sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also scrape a webpage using the `requests` and `lxml` libraries. Let's trying scraping a paragraph from the landing page for IBM Think. Use 'Inspect' functionality of your browser for the webpage to get the XPath for a particular element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "# Scraping data from a webpage element\n",
    "page = requests.get('https://www.ibm.com/events/think')\n",
    "tree = html.fromstring(page.content)\n",
    "webpage_data = tree.xpath('//*[@id=\"introSection\"]/div/div/div/p/text()') \n",
    "\n",
    "# Iterating over all the elements\n",
    "conference_data = []\n",
    "for item in webpage_data:\n",
    "    conference_data.append(item)\n",
    "print('\\nConference data:', conference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation\n",
    "A paragraph is nothing but a collection of sentences. Also called sentence tokenization or sentence boundary disambiguation, this process breaks up sentences by deciding where a sentence starts and ends. Challenges include recognizing ambiguous puncutation marks. For example, `.` can be used for a decimal point, an ellipsis or a period. Let's use ```sent_tokenize``` from ```nltk.tokenize``` to get sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def get_sent_tokens(data):\n",
    "    \"\"\"Sentence tokenization\"\"\"\n",
    "    sentences = []\n",
    "    for sent in data:\n",
    "        sentences.extend(sent_tokenize(sent))\n",
    "    print('\\nSentence tokens:', sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = get_sent_tokens(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_sentences = get_sent_tokens(conference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization\n",
    "A sentence is a collection of words. Word tokenization is similar to sentence tokenization, but works on words. Let's use ```word_tokenize``` from ```nltk.tokenize``` to get the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_word_tokens(sentences):\n",
    "    '''Word tokenization'''\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        words.extend(word_tokenize(sent))\n",
    "    print('\\nWord tokens:', words)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_words = get_word_tokens(sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_words = get_word_tokens(conference_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution\n",
    "Calculates the frequency distribution for each word in the data. Use ```nltk.probability``` from ```FreqDist``` and ```matplotlib```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from nltk.probability import FreqDist\n",
    "matplotlib.use('TkAgg') \n",
    "\n",
    "def plot_freq_dist(words, num_words = 20):\n",
    "    '''Frequency distribution'''\n",
    "    fdist = FreqDist(words)\n",
    "    fdist.plot(num_words, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_freq_dist(sample_words, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq_dist(conference_words, num_words=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "![Cleaning data](https://media.giphy.com/media/10zsjaH4g0GgmY/giphy.gif)\n",
    "\n",
    "Oops, we missed a crucial step! Real world data is often messy and needs to undergo cleaning. You can do a bunch of preprocessing to ensure the data is clean, like:\n",
    "- Removing special characters and numbers - These are usually not important when trying to derive the semantics\n",
    "- Removing stopwords - A special category of words that don't have any significance on their own and are often used as filler words or to ensure correct grammer. Eg. the, and, but, of, is, or, those, her, \n",
    "- Removing HTML tags - Raw data from webpages can often be laden with HTML tags. Use a library like `BeautifulSoup` to process and remove the tags.\n",
    "- Standardizing words - This aims to consolidate different versions of the same version Eg. SMS/Twitter language, slang, misspellings \n",
    "- Converting to lower case - To ensure uniformity across all words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_clean_sentences(sentences, remove_digits=False):\n",
    "    '''Cleaning sentences by removing special characters and optionally digits'''\n",
    "    clean_sentences = []\n",
    "    for sent in sentences:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]' \n",
    "        clean_text = re.sub(pattern, '', sent)\n",
    "        clean_text = clean_text.lower()  # Converting to lower case\n",
    "        clean_sentences.append(clean_text)\n",
    "    print('\\nClean sentences:', clean_sentences)\n",
    "    return clean_sentences\n",
    "\n",
    "def filter_stopwords(words):\n",
    "    '''Removing stopwords from given words'''\n",
    "    filtered_words = [w for w in words if w not in stop_words]\n",
    "    print('\\nFiltered words:', filtered_words)\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = get_clean_sentences(sample_sentences, remove_digits = True)\n",
    "sample_words = get_word_tokens(sample_sentences)\n",
    "sample_words = filter_stopwords(sample_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_sentences = get_clean_sentences(conference_sentences)\n",
    "conference_words = get_word_tokens(conference_sentences)\n",
    "conference_words = filter_stopwords(conference_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the text and using tokenization, we are left with words. Words have certain properties which we'll be exploring in the next few sections. These characteristics can often be used as features for a Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "\n",
    "The English language is formed of different parts of speech (POS) like nouns, verbs, pronouns, adjectives, etc. POS tagging analyzes the words in a sentences and associates it with a POS tag depending on the way it is used. Also called grammatical tagging or word-category disambiguation. Use ```nltk.pos_tag``` for the process. There are different types of tagsets used with the most common being the Penn Treebank tagset and the Universal tagset. \n",
    "\n",
    "![Penn POS tags](https://slideplayer.com/slide/6855236/23/images/11/Penn+TreeBank+POS+Tag+set.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_pos_tags(words):\n",
    "    '''Get the part of speech (POS) tags for the words'''\n",
    "    tags=[]\n",
    "    for word in words:\n",
    "        tags.append(nltk.pos_tag([word]))\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tags = get_pos_tags(sample_words)\n",
    "sample_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conference_tags = get_pos_tags(conference_words)\n",
    "conference_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing\n",
    "Text processing approaches like stemming and lemmatization help in reducing inflectional forms of words. \n",
    "### Dictionary and thesaurus\n",
    "WordNet is a lexical database that also has relationships between different words. You can use synsets to find definitions, synonyms and antonyms for words. You can also find hyponyms and hypernyms using the same process. Hypernym is a generalized concept like 'programming language' whereas hyponym is a specific concept like 'Python' or 'Java'.\n",
    "\n",
    "![Hypernym and hyponym](https://upload.wikimedia.org/wikipedia/en/thumb/1/1f/Hyponymsandhypernyms.jpg/300px-Hyponymsandhypernyms.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "def get_wordnet_properties(words):\n",
    "    '''Returns definition, synonyms and antonyms of words'''\n",
    "    for word in words:\n",
    "        synonyms = []\n",
    "        antonyms = []\n",
    "#         hyponyms = []\n",
    "#         hypernyms = []\n",
    "        definitions = []\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lm in syn.lemmas():\n",
    "                synonyms.append(lm.name())\n",
    "                if lm.antonyms(): \n",
    "                    antonyms.append(lm.antonyms()[0].name())\n",
    "#             hyponyms.append(syn.hyponyms())\n",
    "#             hypernyms.append(syn.hypernyms())\n",
    "#             definitions.append(syn.definition())\n",
    "            \n",
    "        print(word)\n",
    "        print('Synonyms:', synonyms, '\\nAntonyms:', antonyms, '\\n')\n",
    "#         print('Definition:', definitions, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you watched the series 'Friends'? Do you remember the [episode](https://youtu.be/B1tOqZUNebs?t=100) where Joey has to write a letter of recommendation for Monica and Chandler for the adoption agency? He uses a thesaurus to make himself sound smarter in the letter! Let's see if we get the same results:\n",
    "\n",
    "'They are warm, nice people with big hearts' -> 'They are humid, prepossessing Homo Sapiens with full-sized aortic pumps'\n",
    "\n",
    "![Joey Friends](https://media.giphy.com/media/VEsfbW0pBu145PPhOi/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joey_dialogue = ['they', 'are', 'warm', 'nice', 'people', 'with', 'big', 'hearts']\n",
    "get_wordnet_properties(joey_dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation\n",
    "\n",
    "These synsets are also used for disambiguation, particularly Word Sense Disambiguation using Lesk Algorithm. See: http://www.nltk.org/howto/wsd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "sent = ['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.']\n",
    "print(lesk(sent, 'bank', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['I', 'was', 'sitting', 'by', 'the', 'bank', '.']\n",
    "print(lesk(sent, 'bank', 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming tries to cut off at the ends of the words in the hope of deriving the base form. Stems aren't always real words. Use ```PorterStemmer``` from ```ntlk.stem```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def get_stems(words):\n",
    "    '''Reduce the words to their base word (stem) by cutting off the ends'''\n",
    "    ps = PorterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stems.append(ps.stem(word))\n",
    "    print(stems)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_stems = get_stems(sample_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_stems = get_stems(conference_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Lemmatization groups different inflected forms of a words so they can be mapped to the same base. Lemmas are real words. More complex than stemming, context of words is also analyzed. Uses WordNet which is a lexical English database. \n",
    "Use ```WordNetLemmatizer``` from ```nltk.stem``` and provide it the POS tag along with the word. NLTK’s POS tags are in a format different from to that of wordnet lemmatizer, so a mapping is needed. https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_lemma(word_tags):\n",
    "    '''Reduce the words to their base word (lemma) by using a lexicon'''\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemma = []\n",
    "    for element in word_tags:\n",
    "        word = element[0][0]\n",
    "        pos = element[0][1]\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ, # Mapping NLTK POS tags to WordNet POS tags\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "        wordnet_pos = tag_dict.get(tag, wordnet.NOUN)\n",
    "        lemma.append(wordnet_lemmatizer.lemmatize(word, wordnet_pos))\n",
    "    print(lemma)\n",
    "    return(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_lemma = get_lemma(sample_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conference_lemma = get_lemma(conference_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These processes can create features that act as inputs to predictive models. It also helps in using lesser memory by making the data smaller and reducing the size of the vocabulary. Often times, these normalized words are sufficient to provide the semantics. Like in the case of understanding the meaning behind the sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances \n",
    "You can calculate distances between words. There are a variety of distance metrics available: https://en.wikipedia.org/wiki/String_metric. The most common ones are Levenshtein, Cosine distances and Jaccard similarity. Applications include spell checking, correction for OCRs and Machine Translation. For an implementation of a spell checker, see here: https://norvig.com/spell-correct.html\n",
    "\n",
    "![Edit distance](https://i.stack.imgur.com/5Pjr7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) \n",
    "\n",
    "Also known as entity chunking or extraction, is a sub-process of information extraction. This involves identifies and classifies named entities mentions into sub-categories like person name, organization, location, time, etc.  In other words, Named Entity Recognition (NER) labels sequences of words in a text which are the names of things, such as person and company names, or gene and protein names. \n",
    "\n",
    "Some of the most popular NER models are here: https://towardsdatascience.com/a-review-of-named-entity-recognition-ner-using-automatic-summarization-of-resumes-5248a75de175. <br>\n",
    "\n",
    "Example use-cases include customer support, search engine, news classification. Another emerging application is for redacting personally identifiable information (PII). A great demo of NER in action is here: https://explosion.ai/demos/displacy-ent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text representation\n",
    "### Bag of words\n",
    "Bag of words is an approach for text feature extraction. Just imagine a bag of popcorn, \n",
    "and each popcorn kernel represents a word that is present in the text. Each sentence can be represented as a vector\n",
    "of all the words present in a vocabulary. If a word is present in the sentence, it is 1, otherwise 0.\n",
    "\n",
    "![Bag of words](https://cdn-images-1.medium.com/max/1600/1*zMdHVQQ7HYv_mMZ5Ne-2yQ.png)\n",
    "\n",
    "### TF-IDF\n",
    "Term-frequency inverse document frequency assigns scores to words inside a document. Commonly occuring words in all documents would have less weightage.\n",
    "![TF IDF](http://www.bloter.net/wp-content/uploads/2016/09/td-idf-graphic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_bag_of_words(sentences):\n",
    "    '''Compute bag of words for sentences'''\n",
    "    vectorizer = CountVectorizer()\n",
    "    print('\\nBag of words:', vectorizer.fit_transform(sentences).todense())\n",
    "    print('\\nDictionary:', vectorizer.vocabulary_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bag_of_words(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_bag_of_words(conference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings - Word2Vec\n",
    "Vector space model - represent words and sentences as vectors to get semantic relationships. A really good tutorial for Word2Vec is here: https://www.kaggle.com/alvations/word2vec-embedding-using-gensim-and-nltk\n",
    "\n",
    "![Word2Vec](http://www.flyml.net/wp-content/uploads/2016/11/w2v-3-samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning using Natural Language Processing\n",
    "Machine Learning includes two approaches: supervised and unsupervised. Supervised learning works on data that already has labels i.e. they provide supervision to the model. Eg. Classification, Regression. Unsupervised learning is to find out the inherent structure present in the data and there are no labels i.e. no supervision. Eg. Clustering.\n",
    "\n",
    "A lot of these text properties can be used as features for Machine Learning systems. One specific case is text classification. A more detailed resource is here: https://www.nltk.org/book/ch06.html\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics. More details here: https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "print('Classes present:', news.target_names)\n",
    "print('Number of classes present:', len(news.target_names))\n",
    "print('Number of data points:', len(news.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Printing the first few characters for each category\n",
    "\n",
    "for text, num_label in zip(news.data[:10], news.target[:10]):\n",
    "    print('[%s]:\\t\\t \"%s ...\"' % (news.target_names[num_label], text[:500].split('\\n')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing into training and test data sets\n",
    "Think of it as learning in class (training) and then taking an exam (testing) to evaluate your performance. The testing is done on unseen data to know the actual abilities of the classifier i.e. preventing memorization or rote-learning. Test data set is usually 20-25% of the data set. You can also use cross-validation to ensure robustness of classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "def train(classifier, X, y):\n",
    "    \"\"\"Train given classifier\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"\\nAccuracy:\", classifier.score(X_test, y_test))\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Probabilistic classifier based on Bayes theorem. Assumes independence among the features. Details here: https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "\n",
    "We'll be using Pipeline to apply transformations sequentially: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "Adapted from: https://nlpforhackers.io/text-classification/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "model_1 = Pipeline([('vectorizer', TfidfVectorizer()),('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results for TF-IDF as feature:')\n",
    "train(model_1, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: TF i.e. Removing IDF from TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Pipeline([('vectorizer', TfidfVectorizer(use_idf=False)),\n",
    "                    ('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results for TF as feature, removing IDF')\n",
    "train(model_2, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, IDF does make a huge difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: TF-IDF + stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))),\n",
    "                    ('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Results for TF-IDF as feature, using stopword removal:')\n",
    "train(model_3, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: TF-IDF + stopwords removal + ignoring words with frequency < 5\n",
    "Trying simple things may work too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'), min_df=5)),\n",
    "                    ('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Results for TF-IDF + stopwords removal + ignoring words with frequency < 5:')\n",
    "train(model_4, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: TF-IDF + stopwords removal + ignoring words with frequency < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'), min_df=10)),\n",
    "                    ('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Results for TF-IDF + stopwords removal + ignoring words with frequency < 10:')\n",
    "train(model_5, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to not go overboard with simple steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: TF-IDF + stopwords removal + ignoring words with frequency < 5 + tuning hyperparameter alpha\n",
    "Alpha is a hyperparameter for smoothing in Multinomial NB that controls the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "model_6 = Pipeline([('vectorizer', TfidfVectorizer(min_df = 5,\n",
    "                     stop_words=stopwords.words('english') + list(string.punctuation))),\n",
    "                   ('classifier', MultinomialNB(alpha=0.05))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Results for TF-IDF + stopwords removal + ignoring words with frequency < 5 + tuning hyperparameter alpha:')\n",
    "train(model_6, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: TF-IDF + stopwords removal + ignoring words with frequency < 5 + tuning hyperparameter alpha + stemming\n",
    "Let's check if stemming the words makes any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    " \n",
    "def stem_tokenizer(text):\n",
    "    \"\"\"Computing stem for each word in text\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in word_tokenize(text)]\n",
    "\n",
    "# Stemming the stopwords as text is stemmed first, and then stopwords are removed\n",
    "stemmed_stopwords = [PorterStemmer().stem(word) for word in stopwords.words('english')]\n",
    " \n",
    "model_7 = Pipeline([('vectorizer', TfidfVectorizer(tokenizer=stem_tokenizer, min_df = 5,\n",
    "                     stop_words=stemmed_stopwords + list(string.punctuation))),\n",
    "                   ('classifier', MultinomialNB(alpha=0.05))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Results for TF-IDF + stopwords removal + ignoring words with frequency < 5 +', \n",
    "      'tuning hyperparameter alpha + stemming:')\n",
    "train(model_7, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to experiment with other features and see how well the classifier performs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "NLTK's VADER algorithm is used to detect polarity of words and establish the overall sentiment (compound score) for sentences. We're using a small sample of IMDB review titles for Captain Marvel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "\n",
    "def get_sentiment(data):\n",
    "    '''Get sentiment of sentences using VADER algorithm'''\n",
    "    scorer = SentimentIntensityAnalyzer()\n",
    "    for sentence in reviews:\n",
    "        print(sentence)\n",
    "        ss = scorer.polarity_scores(sentence)\n",
    "        for k in ss:\n",
    "            print('{0}: {1}, ' .format(k, ss[k]), end='')\n",
    "        print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\"I've watched this movie for the record, cause I had no particular expectations for it.\",\n",
    "          \"It's fine.\",\n",
    "          \"Not bad, not great either.\",\n",
    "          \"Captain Marvel is another fine Marvel adventure.\",\n",
    "          \"Captivating story and great visuals.\",\n",
    "          \"Confusing boring childish.\",\n",
    "          \"Strong Hero. Weak Film.\"]\n",
    "\n",
    "get_sentiment(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "Topic modeling is an unsupervised ML method used to find inherent structure in documents. It learns\n",
    "representations of topics in documents which allows grouping of different documents together. We will\n",
    "use ```Gensim``` library and Latent Dirichlet Allocation (LDA) for this.\n",
    "\n",
    "LDA’s approach to topic modeling is it considers each document as a collection of topics in a certain proportion. And each topic as a collection of keywords, again, in a certain proportion.\n",
    "\n",
    "Once you provide the algorithm with the number of topics, all it does it to rearrange the topics distribution within the documents and keywords distribution within the topics to obtain a good composition of topic-keywords distribution.\n",
    "\n",
    "Adapted from https://kleiber.me/blog/2017/07/22/tutorial-lda-wikipedia/\n",
    "\n",
    "![Topic modeling](https://i.stack.imgur.com/vI8Lc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia, random\n",
    "\n",
    "def fetch_data(article_names):\n",
    "    '''Fetching the data from given Wikipedia articles'''\n",
    "    wikipedia_random_articles = wikipedia.random(2)\n",
    "    wikipedia_random_articles.extend(article_names)\n",
    "    wikipedia_random_articles\n",
    "    print(wikipedia_random_articles)\n",
    "    \n",
    "    wikipedia_articles = []\n",
    "    for wikipedia_article in wikipedia_random_articles:\n",
    "        wikipedia_articles.append([wikipedia_article, \n",
    "                                   wikipedia.page(wikipedia_article).content])\n",
    "    return wikipedia_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')    \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def clean(article):\n",
    "    '''Cleaning the article contents and getting the word stems'''\n",
    "    title, document = article\n",
    "    tokens = RegexpTokenizer(r'\\w+').tokenize(document.lower())\n",
    "    tokens_clean = [token for token in tokens if token not in \n",
    "                    stopwords.words('english')]\n",
    "    tokens_stemmed = [PorterStemmer().stem(token) for token \n",
    "                      in tokens_clean]\n",
    "    return (title, tokens_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "article_names = ['Programming', 'Singing', 'Dancing', \n",
    "                 'Painting', 'Computer']\n",
    "wikipedia_articles = fetch_data(article_names)\n",
    "wikipedia_articles\n",
    "wikipedia_articles_clean = list(map(clean, wikipedia_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_contents = [article[1] for article in wikipedia_articles_clean]\n",
    "dictionary = corpora.Dictionary(article_contents)\n",
    "corpus = [dictionary.doc2bow(article) for article in \n",
    "          article_contents[:-1]] # All except the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=6, \n",
    "                                            id2word = dictionary, \n",
    "                                            passes=100)\n",
    "\n",
    "topic_results = lda_model.print_topics(num_topics=6, num_words=5)\n",
    "topic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "similarity = list(lda_model[[dictionary.doc2bow(article_contents[-1])]])\n",
    "print('Similarity to each of the topics:', similarity[0])\n",
    "match = max(similarity[0], key=itemgetter(1))\n",
    "print('Given topic is most similar to topic', match[0], ' with a similarity of', match[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Thank you for attending! Would appreciate if you could give your [feedback](https://docs.google.com/forms/d/1wh7oz6UA4v3DI30FbRyPmnDqa4hZlJeZUuBDn0JcnE0/prefill). \n",
    "\n",
    "- More on NLP https://monkeylearn.com/blog/definitive-guide-natural-language-processing/\n",
    "- A very comprehensive list of resources by Penn https://www.seas.upenn.edu/~romap/nlp-resources.html\n",
    "- Peter Norvig's spell corrector http://norvig.com/spell-correct.html\n",
    "- Applications and datasets https://machinelearningmastery.com/datasets-natural-language-processing/\n",
    "- More datasets https://gengo.ai/datasets/the-best-25-datasets-for-natural-language-processing/\n",
    "- https://towardsdatascience.com/text-analytics-topic-modelling-on-music-genres-song-lyrics-deb82c86caa2\n",
    "- Collection of tutorials https://medium.com/machine-learning-in-practice/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition-dd8cf53cb7dc\n",
    "- Text classification https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "You can contact me at:\n",
    "\n",
    "- Personal website: https://gjena.github.io/\n",
    "- LinkedIn: https://www.linkedin.com/in/grishmajena/\n",
    "- Twitter: @DebateLover"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
